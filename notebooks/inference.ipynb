{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer, get_cosine_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup, get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import time\n",
    "from torch.utils import checkpoint\n",
    "import math\n",
    "import gc\n",
    "from typing import Dict, List, Tuple\n",
    "import codecs\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass, field, asdict\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "from functools import partial\n",
    "transformers.logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "# declare the two GPUs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n",
    "\n",
    "# avoids some issues when using more than one worker\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class cfg:\n",
    "    test_summary_file: str = field(default=\"/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv\", metadata={\"help\": \"test file path\"})\n",
    "    test_prompt_file: str = field(default=\"/kaggle/input/commonlit-evaluate-student-summaries/prompts_test.csv\", metadata={\"help\": \"test file path\"})\n",
    "    sample_submission_file: str = field(default=\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\", metadata={\"help\": \"sample submission file path\"})\n",
    "    batch_size: int = field(default=16, metadata={\"help\": \"batch size\"})\n",
    "    hidden_dropout_prob: float = field(default=0.0, metadata={\"help\": \"hidden dropout probability\"})\n",
    "    layer_norm_eps: float = field(default=1e-7, metadata={\"help\": \"layer norm eps\"})\n",
    "    target_columns: List[str] = field(default = ('content', 'wording'), metadata={\"help\": \"target columns\"})\n",
    "    num_classes: int = field(default=2, metadata={\"help\": \"number of classes\"})\n",
    "    num_workers: int = field(default=4, metadata={\"help\": \"number of workers\"})\n",
    "    device: str = field(default=\"cuda\" if torch.cuda.is_available() else \"cpu\", metadata={\"help\": \"device\"})\n",
    "    multi_gpu: bool = field(default=torch.cuda.device_count() > 1, metadata={\"help\": \"multi gpu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"Model class\"\"\"\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "        config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"hidden_dropout_prob\": cfg.hidden_dropout_prob,\n",
    "                \"attention_probs_dropout_prob\" : cfg.hidden_dropout_prob,\n",
    "                \"layer_norm_eps\": cfg.layer_norm_eps,\n",
    "                \"add_pooling_layer\": False,\n",
    "                \"num_labels\": cfg.num_classes,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.transformer = AutoModel.from_config(config)\n",
    "        self.output = nn.Linear(config.hidden_size, cfg.num_classes)\n",
    "    \n",
    "    def forward(self, ids, mask, targets = None):\n",
    "        transformer_out = self.transformer(input_ids = ids, attention_mask = mask)\n",
    "        logits = self.output(transformer_out.last_hidden_state[:,0,:])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grps = {\n",
    "    'deberta_v3_512':{\n",
    "        'arch': partial( Model, model_name=\"microsoft/deberta-v3-base\"),\n",
    "        'tokenizer_path': AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\"),\n",
    "        'checkpoints':[\n",
    "           \"../input/colab-models-download-deberta-large/deberta-v3-large/deberta-v3-large_fold_0.pth\",\n",
    "        \"../input/colab-models-download-deberta-large/deberta-v3-large/deberta-v3-large_fold_1.pth\",\n",
    "        \"../input/colab-models-download-deberta-large/deberta-v3-large/deberta-v3-large_fold_2.pth\",\n",
    "        \"../input/colab-models-download-deberta-large/deberta-v3-large/deberta-v3-large_fold_3.pth\",\n",
    "        \"../input/colab-models-download-deberta-large/deberta-v3-large/deberta-v3-large_fold_4.pth\"\n",
    "        ],\n",
    "        'max_length' : 512,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_one_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    model.to(device)\n",
    "    if cfg.multi_gpu:\n",
    "        model = nn.DataParallel(model)\n",
    "    for step, batch in tqdm( enumerate(dataloader), total=len(dataloader)):\n",
    "        \n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        pred.append(outputs.detach().cpu().numpy())\n",
    "    pred = np.concatenate(pred, axis = 0)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_training_data_helper(tokenizer, df, max_len):\n",
    "    training_samples = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row['text']\n",
    "        encoded_text = tokenizer(text, add_special_tokens = True, max_length = max_len, padding = False, truncation = 'longest_first') \n",
    "        sample = {\n",
    "            \"student_id\": row[\"student_id\"],\n",
    "            \"input_ids\": encoded_text['input_ids'],\n",
    "            \"attention_mask\": encoded_text['attention_mask']\n",
    "\n",
    "        }\n",
    "\n",
    "        training_samples.append(sample)\n",
    "    return training_samples\n",
    "\n",
    "\n",
    "def prepare_training_data(df, tokenizer, num_jobs, max_len):\n",
    "    training_samples = []\n",
    "\n",
    "    df_splits = np.array_split(df, num_jobs)\n",
    "\n",
    "    results = Parallel(n_jobs=num_jobs, backend=\"multiprocessing\")(\n",
    "        delayed(_prepare_training_data_helper)( tokenizer, df, max_len) for df in df_splits\n",
    "    )\n",
    "    for result in results:\n",
    "        training_samples.extend(result)\n",
    "\n",
    "    return training_samples\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, samples,  tokenizer):\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"ids\": self.samples[idx][\"input_ids\"],\n",
    "            \"mask\": self.samples[idx][\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n",
    "        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n",
    "        else:\n",
    "            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n",
    "            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n",
    "        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and combine data from test files\n",
    "pdf = pd.read_csv(cfg.test_prompt_file)\n",
    "sdf = pd.read_csv(cfg.test_summary_file)\n",
    "df = pdf.merge(sdf, on=\"prompt_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = []\n",
    "\n",
    "for model_type,model in model_grps.items():\n",
    "    \n",
    "    print(f'Model Type : {model_type}')\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    print('[INFO] Preparing Data...')\n",
    "    \n",
    "    tokenizer = model['tokenizer_path']\n",
    "    samples = prepare_training_data(df, tokenizer, num_jobs=cfg.num_workers, max_len=model['max_length'])\n",
    "    samples = list(sorted(samples, key=lambda d: len(d[\"input_ids\"])))\n",
    "    dataset = Dataset(samples, tokenizer)\n",
    "    collate_fn = Collate(tokenizer)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(dataset, \n",
    "                         batch_size=cfg.batch_size,\n",
    "                         shuffle=False,\n",
    "                         collate_fn = collate_fn,\n",
    "                         num_workers = cfg.num_workers,\n",
    "                         pin_memory = True,\n",
    "                         drop_last = False,\n",
    "                        )\n",
    "    \n",
    "    print('[INFO] Inferring....')\n",
    "    \n",
    "\n",
    "    for chkpoint in model['checkpoints']:\n",
    "        net =  model['arch']()\n",
    "\n",
    "        state = torch.load(chkpoint,\n",
    "                           map_location = torch.device('cpu'))\n",
    "        net.load_state_dict(state)\n",
    "        prediction = inference_one_epoch(net, test_loader, cfg.device)\n",
    "        predictions.append(prediction)\n",
    "        del net, state, prediction\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    final_predictions.append( ( [x[\"student_id\"] for x in samples], np.mean(predictions, axis=0) ) )\n",
    "    \n",
    "    del dataset, samples, test_loader\n",
    "    gc.collect()\n",
    "    \n",
    "    print('[INFO] Inference Complete.\\n\\n')\n",
    "    \n",
    "    predictions.clear()\n",
    "    del tokenizer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(cfg.sample_submission_file)\n",
    "sample_submission = sample_submission.set_index('student_id')\n",
    "\n",
    "target_columns = list(cfg.target_columns)\n",
    "for i, ( ids, preds ) in enumerate( final_predictions ):\n",
    "    \n",
    "    if i == 0:\n",
    "        for k,col in enumerate(target_columns):\n",
    "            sample_submission.loc[ids, col] = preds[:, k]\n",
    " \n",
    "    else:\n",
    "        for k,col in enumerate(target_columns):\n",
    "            sample_submission.loc[ids, col] += preds[:, k]\n",
    "\n",
    "\n",
    "sample_submission = sample_submission.reset_index()\n",
    "sample_submission[target_columns] /= len( final_predictions )\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
